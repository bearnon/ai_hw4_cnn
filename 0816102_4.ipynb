{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "0816102_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bearnon/ai_hw4_cnn/blob/main/0816102_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNf1ZO4_sJK2"
      },
      "source": [
        "# Assignment 4: CNN\n",
        "\n",
        "## Description\n",
        "\n",
        "Implement a Convolutional Neural Network (CNN) classifier to predict whether a given icon image is the real / fake. Where the fake images were generated by TAs with a neural network.\n",
        "\n",
        "- You are not required to use Colab in this assignment, but you have to **submit your source code**.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "- https://lab.djosix.com/icons.zip\n",
        "- 64x64 RGB jpg images\n",
        "\n",
        "\n",
        "```\n",
        "real/           (10000 images)\n",
        "    0000.jpg\n",
        "    0001.jpg\n",
        "    ...\n",
        "    9999.jpg\n",
        "fake/           (10000 images)\n",
        "    0000.jpg\n",
        "    0001.jpg\n",
        "    ...\n",
        "    9999.jpg\n",
        "unknown/        (5350 images, testing set)\n",
        "    0000.jpg\n",
        "    0001.jpg\n",
        "    ...\n",
        "    5349.jpg\n",
        "```\n",
        "\n",
        "- Training set\n",
        "  - 20000 icons in `real/` and `fake/`\n",
        "  - You should predict 1 for icons in `real/` and 0 for icons in `fake/`\n",
        "- Testing set:\n",
        "  - 5350 icons in `unknown/`\n",
        "  - Your score depends on the **accuracy** on this testing set,  \n",
        "    so the prediction of each icon in `unknown/` should be submitted (totally 5350 predictions, see below).\n",
        "\n",
        "\n",
        "## Submission\n",
        "\n",
        "Please upload **2 files** to E3. (`XXXXXXX` is your student ID)\n",
        "\n",
        "1. **`XXXXXXX_4_result.json`**  \n",
        "  This file contains your model prediction for the testing set.  \n",
        "  You must generate this file with the function called `save_predictions()`.\n",
        "2. **`XXXXXXX_4_source.zip`**  \n",
        "  Zip your source code into this archive.\n",
        "\n",
        "\n",
        "## Hints\n",
        "\n",
        "- **Deep Learning Libraries**: You can use any deep learning frameworks (PyTorch, TensorFlow, ...).\n",
        "- **How to implement**: There are many CNN examples for beginners on the internet, e.g. official websites of the above libraries, play with them and their model architectures to abtain high accuracy on testing set.\n",
        "- **GPU/TPU**: Colab provides free TPU/GPU for training speedup, please refer to [this page in `pytut.pdf` on E3](https://i.imgur.com/VsrUh7I.png).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j94Kc1dsLaR"
      },
      "source": [
        "### Include this in your code to generate result file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUgYOZfUMvxl"
      },
      "source": [
        "import json\n",
        "\n",
        "def save_predictions(student_id, predictions):\n",
        "  # Please use this function to generate 'XXXXXXX_4_result.json'\n",
        "  # `predictions` is a list of int (0 or 1; fake=0 and real=1)\n",
        "  # For example, `predictions[0]` is the prediction given \"unknown/0000.jpg\".\n",
        "  # it will be 1 if your model think it is real, else 0 (fake).\n",
        "\n",
        "  assert isinstance(student_id, str)\n",
        "  assert isinstance(predictions, list)\n",
        "  assert len(predictions) == 5350\n",
        "\n",
        "  for y in predictions:\n",
        "    assert y in (0, 1)\n",
        "\n",
        "  with open('{}_4_result.json'.format(student_id), 'w') as f:\n",
        "    json.dump(predictions, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBfZB0YmU4tb"
      },
      "source": [
        "###My Code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us9t80GXh0Cb"
      },
      "source": [
        "####initial:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GzSq_IbDoif",
        "outputId": "937d6102-c0bb-4bc2-e09f-2f7638b75a40"
      },
      "source": [
        "import os, pickle\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "!pip install split-folders\n",
        "import splitfolders\n",
        "\n",
        "# def load_data():\n",
        "if not os.path.exists('data'):\n",
        "  # !mkdir image\n",
        "  os.mkdir(\"data\")\n",
        "  url = 'https://lab.djosix.com/icons.zip'\n",
        "  assert os.system(f'wget -O icons.zip {url}') == 0\n",
        "  assert os.system('unzip icons.zip -d ./data') == 0\n",
        "  os.system(\"cp -R data/unknown test/\")\n",
        "  !rm -r data/unknown\n",
        "  # os.rmdir(\"data/unknown\")\n",
        "  # !rm icons.zip\n",
        "# return fake, real, unknown\n",
        "\n",
        "# print(img_data)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting split-folders\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/5f/3c2b2f7ea5e047c8cdc3bb00ae582c5438fcdbbedcc23b3cc1c2c7aae642/split_folders-0.4.3-py3-none-any.whl\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YY-Pi2lY3OK"
      },
      "source": [
        "splitfolders.ratio('data','datatv',2141,(0.5,0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH02bs5xiGU_"
      },
      "source": [
        "####model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mARLB3VU2UV"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN,self).__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "      nn.Conv2d(3,64,3,1), #convolution\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(True)\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "      nn.Conv2d(64,256,5,2),\n",
        "      nn.BatchNorm2d(256),\n",
        "      nn.ReLU(True)\n",
        "    )\n",
        "    self.conv3 = nn.Sequential(\n",
        "      nn.Conv2d(256,256,3,1),\n",
        "      nn.BatchNorm2d(256),\n",
        "      nn.ReLU(True)\n",
        "    )\n",
        "    self.conv4 = nn.Sequential(\n",
        "      nn.Conv2d(128,64,5,1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(True)\n",
        "    )\n",
        "    self.conv5 = nn.Sequential(\n",
        "      nn.Conv2d(64,64,3,1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(True)\n",
        "    )\n",
        "    self.conv6 = nn.Sequential(\n",
        "      nn.Conv2d(64,64,3,1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(True)\n",
        "    )\n",
        "    # self.conv2 = nn.Conv2d(64,128,5,2)\n",
        "    # self.conv3 = nn.Conv2d(128,128,3,1)\n",
        "    self.dropout1 = nn.Dropout(0.25)\n",
        "    self.dropout2 = nn.Dropout(0.5)\n",
        "    self.fc1 = nn.Linear(1024,64)\n",
        "    self.fc2 = nn.Linear(64,32)\n",
        "    self.fc3 = nn.Linear(32,32)\n",
        "    self.fc4 = nn.Linear(32,2)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.max_pool2d(x,2)\n",
        "    x = self.conv2(x)\n",
        "    x = F.max_pool2d(x,2)\n",
        "    x = self.conv3(x)\n",
        "    # x = self.conv4(x)\n",
        "    # x = self.conv5(x)\n",
        "    # x = self.conv6(x)\n",
        "    x = F.max_pool2d(x,2)\n",
        "    x = self.dropout1(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc4(x)\n",
        "    output = F.log_softmax(x,dim=1)\n",
        "    return output\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4cUclyLDtPf"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(3,64,7,2,padding=3),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(64,64,3,1,padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(64,64,3,1,padding=1),\n",
        "        nn.BatchNorm2d(64)\n",
        "    )\n",
        "    self.conv3a = nn.Sequential(\n",
        "        nn.Conv2d(64,128,3,2,padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(128,128,3,1,padding=1),\n",
        "        nn.BatchNorm2d(128)\n",
        "    )\n",
        "    self.ds3 = nn.Sequential(\n",
        "        nn.Conv2d(64,128,1,2),\n",
        "        nn.BatchNorm2d(128)\n",
        "    )\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(128,128,3,1,padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(128,128,3,1,padding=1),\n",
        "        nn.BatchNorm2d(128)\n",
        "    )\n",
        "    self.conv4a = nn.Sequential(\n",
        "        nn.Conv2d(128,256,3,2,padding=1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(256,256,3,1,padding=1),\n",
        "        nn.BatchNorm2d(256)\n",
        "    )\n",
        "    self.ds4 = nn.Sequential(\n",
        "        nn.Conv2d(128,256,1,2),\n",
        "        nn.BatchNorm2d(256)\n",
        "    )\n",
        "    self.conv4 = nn.Sequential(\n",
        "        nn.Conv2d(256,256,3,1,padding=1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(256,256,3,1,padding=1),\n",
        "        nn.BatchNorm2d(256)\n",
        "    )\n",
        "    self.conv5a = nn.Sequential(\n",
        "        nn.Conv2d(256,512,3,2,padding=1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(512,512,3,1,padding=1),\n",
        "        nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.ds5 = nn.Sequential(\n",
        "        nn.Conv2d(256,512,1,2),\n",
        "        nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.conv5 = nn.Sequential(\n",
        "        nn.Conv2d(512,512,3,1,padding=1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(512,512,3,1,padding=1),\n",
        "        nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(512,2)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.max_pool2d(x,3,stride=2,padding=1)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv2(x)\n",
        "    \n",
        "    d1 = self.ds3(x)\n",
        "    x = self.conv3a(x)\n",
        "    # print(d1.shape)\n",
        "    # print(x.shape)\n",
        "    x += d1\n",
        "    x = F.relu(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv3(x)\n",
        "\n",
        "    d2 = self.ds4(x)\n",
        "    x = self.conv4a(x)\n",
        "    x += d2\n",
        "    x = F.relu(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.conv4(x)\n",
        "\n",
        "    d3 = self.ds5(x)\n",
        "    x = self.conv5a(x)\n",
        "    x += d3\n",
        "    x = F.relu(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.conv5(x)\n",
        "    # print(x.shape)\n",
        "    \n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "    \n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItS9mL_J1xm6"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
        "                  padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n",
        "                  padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "        identity = self.downsample(x)\n",
        "\n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out\n",
        "  def _make_layer(block, inplanes,planes, blocks, stride=1):\n",
        "    downsample = None  \n",
        "    if stride != 1 or inplanes != planes:\n",
        "        downsample = nn.Sequential(            \n",
        "            nn.Conv2d(inplanes, planes, 1, stride, bias=False),\n",
        "            nn.BatchNorm2d(planes),\n",
        "        )\n",
        "    layers = []\n",
        "    layers.append(block(inplanes, planes, stride, downsample))\n",
        "    inplanes = planes\n",
        "    for _ in range(1, blocks):\n",
        "        layers.append(block(inplanes, planes))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "# class ResNet(nn.Module):\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, block, layers, num_classes=1000):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.inplanes = 64\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                            bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    \n",
        "    self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "    \n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.fc = nn.Linear(512 , num_classes)\n",
        "\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "    downsample = None  \n",
        "\n",
        "    if stride != 1 or self.inplanes != planes:\n",
        "        downsample = nn.Sequential(\n",
        "            nn.Conv2d(self.inplanes, planes, 1, stride, bias=False),\n",
        "            nn.BatchNorm2d(planes),\n",
        "        )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "    \n",
        "    self.inplanes = planes\n",
        "    \n",
        "    for _ in range(1, blocks):\n",
        "        layers.append(block(self.inplanes, planes))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)           # 224x224\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)         # 112x112\n",
        "\n",
        "    x = self.layer1(x)          # 56x56\n",
        "    x = self.layer2(x)          # 28x28\n",
        "    x = self.layer3(x)          # 14x14\n",
        "    x = self.layer4(x)          # 7x7\n",
        "\n",
        "    x = self.avgpool(x)         # 1x1\n",
        "    x = torch.flatten(x, 1)     # remove 1 X 1 grid and make vector of tensor shape \n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# def resnet34():\n",
        "# def CNN():\n",
        "#   layers=[3, 4, 6, 3]\n",
        "#   model = ResNet(BasicBlock, layers)\n",
        "#   return model"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chHyJGw_iOga"
      },
      "source": [
        "####train & val:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRkImrFwfLE2"
      },
      "source": [
        "def train(model,device,loader,optimizer,epoch):\n",
        "  model.train()\n",
        "  tloss = 0\n",
        "  tcorrect = 0\n",
        "  for idx,(input,target) in enumerate(loader):\n",
        "    input,target = input.to(device),target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input)\n",
        "    # print(len(loader))\n",
        "    # print(output,target)\n",
        "    # tloss += F.nll_loss(output,target,reduction='sum').item()\n",
        "    tloss += F.cross_entropy(output,target,reduction='sum').item()\n",
        "    pred = output.argmax(dim=1,keepdim=True)\n",
        "    tcorrect += pred.eq(target.view_as(pred)).sum().item()\n",
        "    # loss = F.nll_loss(output,target)\n",
        "    loss = F.cross_entropy(output,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if idx%1000 == 0:\n",
        "    # if idx<100:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format( epoch, idx * len(input), len(loader.dataset), 100. * idx / len(loader), loss.item()))\n",
        "  tloss /= len(loader.dataset)\n",
        "  print('Train: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f})'.format(tloss,tcorrect,len(loader.dataset),tcorrect/len(loader.dataset)))\n",
        "  \n",
        "  \n",
        "def val(model,device,loader):\n",
        "  model.eval()\n",
        "  vloss = 0\n",
        "  vcorrect = 0\n",
        "  with torch.no_grad():\n",
        "    for input,target in loader:\n",
        "      input,target = input.to(device),target.to(device)\n",
        "      output = model(input)\n",
        "      # vloss += F.nll_loss(output,target,reduction='sum').item()\n",
        "      vloss += F.cross_entropy(output,target,reduction='sum').item()\n",
        "      pred = output.argmax(dim=1,keepdim=True)\n",
        "      # print(target)\n",
        "      # print(pred)\n",
        "      # print(tgvasp)\n",
        "      # print(pred.eq(target.view_as(pred)).sum().item())\n",
        "      vcorrect += pred.eq(target.view_as(pred)).sum().item()\n",
        "      # correct += temp\n",
        "  vloss /= len(loader.dataset)\n",
        "  print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format( vloss, vcorrect, len(loader.dataset), 100. * vcorrect / len(loader.dataset)))\n",
        "\n",
        "    ### if\n",
        "    \n",
        "    # print('Train Epoch: ',epoch)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TWpflADJe3zi",
        "outputId": "d45f1e6b-5abc-4f79-b303-443bbfbd5ec5"
      },
      "source": [
        "# tran = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor(),transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])])\n",
        "# tv_dataset = {x: datasets.ImageFolder(os.path.join('datatv',x),tran) for x in ['train','val']}\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "tv_dataset = {x: datasets.ImageFolder(os.path.join('datatv',x),transforms.ToTensor()) for x in ['train','val']}\n",
        "\n",
        "batchSize = 100\n",
        "for ep in range(5):\n",
        "  tvdataloader = {x: torch.utils.data.DataLoader(tv_dataset[x],batch_size=batchSize,shuffle=True,num_workers=2) for x in ['train','val']}\n",
        "  datasize = {x: len(tv_dataset[x]) for x in ['train','val']}\n",
        "  \n",
        "  # model = CNN().to(device)\n",
        "  model = CNN(BasicBlock,[3,4,6,3]).to(device)\n",
        "  optimizer = optim.Adam(model.parameters(),lr=0.0002)\n",
        "  \n",
        "  for epoch in range(100):\n",
        "    trainmodel = train(model,device,tvdataloader['train'],optimizer,epoch)\n",
        "    valmodel = val(model,device,tvdataloader['val'])\n",
        "  "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Train Epoch: 1 [0/16000 (0%)]\tLoss: 6.919256\n",
            "Train Epoch: 1 [1000/16000 (6%)]\tLoss: 1.545968\n",
            "Train Epoch: 1 [2000/16000 (12%)]\tLoss: 0.634542\n",
            "Train Epoch: 1 [3000/16000 (19%)]\tLoss: 0.829269\n",
            "Train Epoch: 1 [4000/16000 (25%)]\tLoss: 0.804374\n",
            "Train Epoch: 1 [5000/16000 (31%)]\tLoss: 0.856872\n",
            "Train Epoch: 1 [6000/16000 (38%)]\tLoss: 0.644196\n",
            "Train Epoch: 1 [7000/16000 (44%)]\tLoss: 0.639166\n",
            "Train Epoch: 1 [8000/16000 (50%)]\tLoss: 0.492655\n",
            "Train Epoch: 1 [9000/16000 (56%)]\tLoss: 1.030082\n",
            "Train Epoch: 1 [10000/16000 (62%)]\tLoss: 0.755635\n",
            "Train Epoch: 1 [11000/16000 (69%)]\tLoss: 0.697282\n",
            "Train Epoch: 1 [12000/16000 (75%)]\tLoss: 0.641386\n",
            "Train Epoch: 1 [13000/16000 (81%)]\tLoss: 0.784478\n",
            "Train Epoch: 1 [14000/16000 (88%)]\tLoss: 0.683270\n",
            "Train Epoch: 1 [15000/16000 (94%)]\tLoss: 0.664279\n",
            "Train: Average loss: 0.7239, Accuracy: 7962/16000 (0.4976)\n",
            "\n",
            "Val set: Average loss: 1.1306, Accuracy: 2000/4000 (50%)\n",
            "\n",
            "Train Epoch: 2 [0/16000 (0%)]\tLoss: 0.575998\n",
            "Train Epoch: 2 [1000/16000 (6%)]\tLoss: 0.646160\n",
            "Train Epoch: 2 [2000/16000 (12%)]\tLoss: 0.745266\n",
            "Train Epoch: 2 [3000/16000 (19%)]\tLoss: 0.651346\n",
            "Train Epoch: 2 [4000/16000 (25%)]\tLoss: 0.732333\n",
            "Train Epoch: 2 [5000/16000 (31%)]\tLoss: 0.726417\n",
            "Train Epoch: 2 [6000/16000 (38%)]\tLoss: 0.663017\n",
            "Train Epoch: 2 [7000/16000 (44%)]\tLoss: 0.783325\n",
            "Train Epoch: 2 [8000/16000 (50%)]\tLoss: 0.747213\n",
            "Train Epoch: 2 [9000/16000 (56%)]\tLoss: 0.610455\n",
            "Train Epoch: 2 [10000/16000 (62%)]\tLoss: 0.806871\n",
            "Train Epoch: 2 [11000/16000 (69%)]\tLoss: 0.712375\n",
            "Train Epoch: 2 [12000/16000 (75%)]\tLoss: 0.725758\n",
            "Train Epoch: 2 [13000/16000 (81%)]\tLoss: 0.750917\n",
            "Train Epoch: 2 [14000/16000 (88%)]\tLoss: 0.517461\n",
            "Train Epoch: 2 [15000/16000 (94%)]\tLoss: 0.711719\n",
            "Train: Average loss: 0.6962, Accuracy: 7982/16000 (0.4989)\n",
            "\n",
            "Val set: Average loss: 0.6970, Accuracy: 2000/4000 (50%)\n",
            "\n",
            "Train Epoch: 3 [0/16000 (0%)]\tLoss: 0.602995\n",
            "Train Epoch: 3 [1000/16000 (6%)]\tLoss: 0.702882\n",
            "Train Epoch: 3 [2000/16000 (12%)]\tLoss: 0.683927\n",
            "Train Epoch: 3 [3000/16000 (19%)]\tLoss: 0.671717\n",
            "Train Epoch: 3 [4000/16000 (25%)]\tLoss: 0.813957\n",
            "Train Epoch: 3 [5000/16000 (31%)]\tLoss: 0.684457\n",
            "Train Epoch: 3 [6000/16000 (38%)]\tLoss: 0.530573\n",
            "Train Epoch: 3 [7000/16000 (44%)]\tLoss: 0.773067\n",
            "Train Epoch: 3 [8000/16000 (50%)]\tLoss: 0.700259\n",
            "Train Epoch: 3 [9000/16000 (56%)]\tLoss: 0.761091\n",
            "Train Epoch: 3 [10000/16000 (62%)]\tLoss: 0.731226\n",
            "Train Epoch: 3 [11000/16000 (69%)]\tLoss: 0.650258\n",
            "Train Epoch: 3 [12000/16000 (75%)]\tLoss: 0.499256\n",
            "Train Epoch: 3 [13000/16000 (81%)]\tLoss: 0.685868\n",
            "Train Epoch: 3 [14000/16000 (88%)]\tLoss: 0.473089\n",
            "Train Epoch: 3 [15000/16000 (94%)]\tLoss: 0.670340\n",
            "Train: Average loss: 0.6950, Accuracy: 8230/16000 (0.5144)\n",
            "\n",
            "Val set: Average loss: 2.2946, Accuracy: 2157/4000 (54%)\n",
            "\n",
            "Train Epoch: 4 [0/16000 (0%)]\tLoss: 0.636253\n",
            "Train Epoch: 4 [1000/16000 (6%)]\tLoss: 0.602872\n",
            "Train Epoch: 4 [2000/16000 (12%)]\tLoss: 0.604959\n",
            "Train Epoch: 4 [3000/16000 (19%)]\tLoss: 0.594898\n",
            "Train Epoch: 4 [4000/16000 (25%)]\tLoss: 0.716399\n",
            "Train Epoch: 4 [5000/16000 (31%)]\tLoss: 0.527095\n",
            "Train Epoch: 4 [6000/16000 (38%)]\tLoss: 0.460787\n",
            "Train Epoch: 4 [7000/16000 (44%)]\tLoss: 0.859224\n",
            "Train Epoch: 4 [8000/16000 (50%)]\tLoss: 0.392382\n",
            "Train Epoch: 4 [9000/16000 (56%)]\tLoss: 1.156415\n",
            "Train Epoch: 4 [10000/16000 (62%)]\tLoss: 1.101028\n",
            "Train Epoch: 4 [11000/16000 (69%)]\tLoss: 0.535922\n",
            "Train Epoch: 4 [12000/16000 (75%)]\tLoss: 0.617535\n",
            "Train Epoch: 4 [13000/16000 (81%)]\tLoss: 0.373708\n",
            "Train Epoch: 4 [14000/16000 (88%)]\tLoss: 1.054785\n",
            "Train Epoch: 4 [15000/16000 (94%)]\tLoss: 0.295319\n",
            "Train: Average loss: 0.6752, Accuracy: 9303/16000 (0.5814)\n",
            "\n",
            "Val set: Average loss: 0.7511, Accuracy: 2329/4000 (58%)\n",
            "\n",
            "Train Epoch: 5 [0/16000 (0%)]\tLoss: 0.605553\n",
            "Train Epoch: 5 [1000/16000 (6%)]\tLoss: 0.510938\n",
            "Train Epoch: 5 [2000/16000 (12%)]\tLoss: 0.424187\n",
            "Train Epoch: 5 [3000/16000 (19%)]\tLoss: 0.422289\n",
            "Train Epoch: 5 [4000/16000 (25%)]\tLoss: 1.086911\n",
            "Train Epoch: 5 [5000/16000 (31%)]\tLoss: 0.520652\n",
            "Train Epoch: 5 [6000/16000 (38%)]\tLoss: 0.402793\n",
            "Train Epoch: 5 [7000/16000 (44%)]\tLoss: 0.513533\n",
            "Train Epoch: 5 [8000/16000 (50%)]\tLoss: 0.145413\n",
            "Train Epoch: 5 [9000/16000 (56%)]\tLoss: 0.277476\n",
            "Train Epoch: 5 [10000/16000 (62%)]\tLoss: 0.177959\n",
            "Train Epoch: 5 [11000/16000 (69%)]\tLoss: 0.517553\n",
            "Train Epoch: 5 [12000/16000 (75%)]\tLoss: 0.518998\n",
            "Train Epoch: 5 [13000/16000 (81%)]\tLoss: 0.665009\n",
            "Train Epoch: 5 [14000/16000 (88%)]\tLoss: 0.287861\n",
            "Train Epoch: 5 [15000/16000 (94%)]\tLoss: 0.689133\n",
            "Train: Average loss: 0.6308, Accuracy: 10419/16000 (0.6512)\n",
            "\n",
            "Val set: Average loss: 0.6104, Accuracy: 2776/4000 (69%)\n",
            "\n",
            "Train Epoch: 6 [0/16000 (0%)]\tLoss: 0.452063\n",
            "Train Epoch: 6 [1000/16000 (6%)]\tLoss: 0.459185\n",
            "Train Epoch: 6 [2000/16000 (12%)]\tLoss: 0.381472\n",
            "Train Epoch: 6 [3000/16000 (19%)]\tLoss: 0.944856\n",
            "Train Epoch: 6 [4000/16000 (25%)]\tLoss: 0.431614\n",
            "Train Epoch: 6 [5000/16000 (31%)]\tLoss: 2.258647\n",
            "Train Epoch: 6 [6000/16000 (38%)]\tLoss: 0.514053\n",
            "Train Epoch: 6 [7000/16000 (44%)]\tLoss: 1.216871\n",
            "Train Epoch: 6 [8000/16000 (50%)]\tLoss: 0.264533\n",
            "Train Epoch: 6 [9000/16000 (56%)]\tLoss: 0.182113\n",
            "Train Epoch: 6 [10000/16000 (62%)]\tLoss: 1.698067\n",
            "Train Epoch: 6 [11000/16000 (69%)]\tLoss: 0.152673\n",
            "Train Epoch: 6 [12000/16000 (75%)]\tLoss: 1.588146\n",
            "Train Epoch: 6 [13000/16000 (81%)]\tLoss: 0.303244\n",
            "Train Epoch: 6 [14000/16000 (88%)]\tLoss: 0.466260\n",
            "Train Epoch: 6 [15000/16000 (94%)]\tLoss: 0.433932\n",
            "Train: Average loss: 0.5896, Accuracy: 11020/16000 (0.6887)\n",
            "\n",
            "Val set: Average loss: 0.5821, Accuracy: 2819/4000 (70%)\n",
            "\n",
            "Train Epoch: 7 [0/16000 (0%)]\tLoss: 0.212518\n",
            "Train Epoch: 7 [1000/16000 (6%)]\tLoss: 0.348394\n",
            "Train Epoch: 7 [2000/16000 (12%)]\tLoss: 0.226299\n",
            "Train Epoch: 7 [3000/16000 (19%)]\tLoss: 0.313230\n",
            "Train Epoch: 7 [4000/16000 (25%)]\tLoss: 0.152518\n",
            "Train Epoch: 7 [5000/16000 (31%)]\tLoss: 0.746490\n",
            "Train Epoch: 7 [6000/16000 (38%)]\tLoss: 0.275802\n",
            "Train Epoch: 7 [7000/16000 (44%)]\tLoss: 1.177462\n",
            "Train Epoch: 7 [8000/16000 (50%)]\tLoss: 0.119040\n",
            "Train Epoch: 7 [9000/16000 (56%)]\tLoss: 0.155811\n",
            "Train Epoch: 7 [10000/16000 (62%)]\tLoss: 1.450918\n",
            "Train Epoch: 7 [11000/16000 (69%)]\tLoss: 0.470370\n",
            "Train Epoch: 7 [12000/16000 (75%)]\tLoss: 0.220313\n",
            "Train Epoch: 7 [13000/16000 (81%)]\tLoss: 0.617593\n",
            "Train Epoch: 7 [14000/16000 (88%)]\tLoss: 0.179549\n",
            "Train Epoch: 7 [15000/16000 (94%)]\tLoss: 0.237824\n",
            "Train: Average loss: 0.5541, Accuracy: 11450/16000 (0.7156)\n",
            "\n",
            "Val set: Average loss: 0.5772, Accuracy: 2742/4000 (69%)\n",
            "\n",
            "Train Epoch: 8 [0/16000 (0%)]\tLoss: 1.057856\n",
            "Train Epoch: 8 [1000/16000 (6%)]\tLoss: 0.462937\n",
            "Train Epoch: 8 [2000/16000 (12%)]\tLoss: 0.301611\n",
            "Train Epoch: 8 [3000/16000 (19%)]\tLoss: 0.431901\n",
            "Train Epoch: 8 [4000/16000 (25%)]\tLoss: 1.400897\n",
            "Train Epoch: 8 [5000/16000 (31%)]\tLoss: 0.200437\n",
            "Train Epoch: 8 [6000/16000 (38%)]\tLoss: 0.277950\n",
            "Train Epoch: 8 [7000/16000 (44%)]\tLoss: 0.140167\n",
            "Train Epoch: 8 [8000/16000 (50%)]\tLoss: 0.302418\n",
            "Train Epoch: 8 [9000/16000 (56%)]\tLoss: 0.897304\n",
            "Train Epoch: 8 [10000/16000 (62%)]\tLoss: 1.193065\n",
            "Train Epoch: 8 [11000/16000 (69%)]\tLoss: 1.333251\n",
            "Train Epoch: 8 [12000/16000 (75%)]\tLoss: 0.065764\n",
            "Train Epoch: 8 [13000/16000 (81%)]\tLoss: 0.024033\n",
            "Train Epoch: 8 [14000/16000 (88%)]\tLoss: 1.261003\n",
            "Train Epoch: 8 [15000/16000 (94%)]\tLoss: 0.821528\n",
            "Train: Average loss: 0.5256, Accuracy: 11863/16000 (0.7414)\n",
            "\n",
            "Val set: Average loss: 0.5955, Accuracy: 2541/4000 (64%)\n",
            "\n",
            "Train Epoch: 9 [0/16000 (0%)]\tLoss: 1.098608\n",
            "Train Epoch: 9 [1000/16000 (6%)]\tLoss: 0.246171\n",
            "Train Epoch: 9 [2000/16000 (12%)]\tLoss: 0.150706\n",
            "Train Epoch: 9 [3000/16000 (19%)]\tLoss: 1.798708\n",
            "Train Epoch: 9 [4000/16000 (25%)]\tLoss: 0.395158\n",
            "Train Epoch: 9 [5000/16000 (31%)]\tLoss: 0.398694\n",
            "Train Epoch: 9 [6000/16000 (38%)]\tLoss: 0.075899\n",
            "Train Epoch: 9 [7000/16000 (44%)]\tLoss: 0.583398\n",
            "Train Epoch: 9 [8000/16000 (50%)]\tLoss: 0.095596\n",
            "Train Epoch: 9 [9000/16000 (56%)]\tLoss: 0.135073\n",
            "Train Epoch: 9 [10000/16000 (62%)]\tLoss: 0.969844\n",
            "Train Epoch: 9 [11000/16000 (69%)]\tLoss: 0.022123\n",
            "Train Epoch: 9 [12000/16000 (75%)]\tLoss: 0.426833\n",
            "Train Epoch: 9 [13000/16000 (81%)]\tLoss: 0.189039\n",
            "Train Epoch: 9 [14000/16000 (88%)]\tLoss: 1.542507\n",
            "Train Epoch: 9 [15000/16000 (94%)]\tLoss: 0.981441\n",
            "Train: Average loss: 0.4211, Accuracy: 12969/16000 (0.8106)\n",
            "\n",
            "Val set: Average loss: 0.5075, Accuracy: 2813/4000 (70%)\n",
            "\n",
            "Train Epoch: 10 [0/16000 (0%)]\tLoss: 0.019083\n",
            "Train Epoch: 10 [1000/16000 (6%)]\tLoss: 0.017577\n",
            "Train Epoch: 10 [2000/16000 (12%)]\tLoss: 0.376602\n",
            "Train Epoch: 10 [3000/16000 (19%)]\tLoss: 0.099211\n",
            "Train Epoch: 10 [4000/16000 (25%)]\tLoss: 0.012876\n",
            "Train Epoch: 10 [5000/16000 (31%)]\tLoss: 0.066313\n",
            "Train Epoch: 10 [6000/16000 (38%)]\tLoss: 0.004627\n",
            "Train Epoch: 10 [7000/16000 (44%)]\tLoss: 0.002138\n",
            "Train Epoch: 10 [8000/16000 (50%)]\tLoss: 0.000499\n",
            "Train Epoch: 10 [9000/16000 (56%)]\tLoss: 0.255308\n",
            "Train Epoch: 10 [10000/16000 (62%)]\tLoss: 0.004223\n",
            "Train Epoch: 10 [11000/16000 (69%)]\tLoss: 0.000777\n",
            "Train Epoch: 10 [12000/16000 (75%)]\tLoss: 0.026227\n",
            "Train Epoch: 10 [13000/16000 (81%)]\tLoss: 0.001195\n",
            "Train Epoch: 10 [14000/16000 (88%)]\tLoss: 0.034751\n",
            "Train Epoch: 10 [15000/16000 (94%)]\tLoss: 0.081240\n",
            "Train: Average loss: 0.2439, Accuracy: 14350/16000 (0.8969)\n",
            "\n",
            "Val set: Average loss: 6.9687, Accuracy: 1125/4000 (28%)\n",
            "\n",
            "Train Epoch: 11 [0/16000 (0%)]\tLoss: 0.012549\n",
            "Train Epoch: 11 [1000/16000 (6%)]\tLoss: 0.002550\n",
            "Train Epoch: 11 [2000/16000 (12%)]\tLoss: 0.020479\n",
            "Train Epoch: 11 [3000/16000 (19%)]\tLoss: 0.067003\n",
            "Train Epoch: 11 [4000/16000 (25%)]\tLoss: 0.000105\n",
            "Train Epoch: 11 [5000/16000 (31%)]\tLoss: 0.007469\n",
            "Train Epoch: 11 [6000/16000 (38%)]\tLoss: 0.000080\n",
            "Train Epoch: 11 [7000/16000 (44%)]\tLoss: 0.002787\n",
            "Train Epoch: 11 [8000/16000 (50%)]\tLoss: 0.019034\n",
            "Train Epoch: 11 [9000/16000 (56%)]\tLoss: 0.003429\n",
            "Train Epoch: 11 [10000/16000 (62%)]\tLoss: 0.019217\n",
            "Train Epoch: 11 [11000/16000 (69%)]\tLoss: 0.000215\n",
            "Train Epoch: 11 [12000/16000 (75%)]\tLoss: 0.166993\n",
            "Train Epoch: 11 [13000/16000 (81%)]\tLoss: 0.000105\n",
            "Train Epoch: 11 [14000/16000 (88%)]\tLoss: 0.000040\n",
            "Train Epoch: 11 [15000/16000 (94%)]\tLoss: 0.000277\n",
            "Train: Average loss: 0.1589, Accuracy: 14945/16000 (0.9341)\n",
            "\n",
            "Val set: Average loss: 1.5489, Accuracy: 2553/4000 (64%)\n",
            "\n",
            "Train Epoch: 12 [0/16000 (0%)]\tLoss: 0.076033\n",
            "Train Epoch: 12 [1000/16000 (6%)]\tLoss: 0.000045\n",
            "Train Epoch: 12 [2000/16000 (12%)]\tLoss: 0.000087\n",
            "Train Epoch: 12 [3000/16000 (19%)]\tLoss: 0.018481\n",
            "Train Epoch: 12 [4000/16000 (25%)]\tLoss: 1.303032\n",
            "Train Epoch: 12 [5000/16000 (31%)]\tLoss: 0.000547\n",
            "Train Epoch: 12 [6000/16000 (38%)]\tLoss: 0.042247\n",
            "Train Epoch: 12 [7000/16000 (44%)]\tLoss: 0.023339\n",
            "Train Epoch: 12 [8000/16000 (50%)]\tLoss: 0.000016\n",
            "Train Epoch: 12 [9000/16000 (56%)]\tLoss: 0.000033\n",
            "Train Epoch: 12 [10000/16000 (62%)]\tLoss: 0.249543\n",
            "Train Epoch: 12 [11000/16000 (69%)]\tLoss: 0.001784\n",
            "Train Epoch: 12 [12000/16000 (75%)]\tLoss: 0.000037\n",
            "Train Epoch: 12 [13000/16000 (81%)]\tLoss: 0.000001\n",
            "Train Epoch: 12 [14000/16000 (88%)]\tLoss: 0.000029\n",
            "Train Epoch: 12 [15000/16000 (94%)]\tLoss: 0.000131\n",
            "Train: Average loss: 0.1154, Accuracy: 15264/16000 (0.9540)\n",
            "\n",
            "Val set: Average loss: 2.2600, Accuracy: 2151/4000 (54%)\n",
            "\n",
            "Train Epoch: 13 [0/16000 (0%)]\tLoss: 0.000196\n",
            "Train Epoch: 13 [1000/16000 (6%)]\tLoss: 0.000270\n",
            "Train Epoch: 13 [2000/16000 (12%)]\tLoss: 0.001987\n",
            "Train Epoch: 13 [3000/16000 (19%)]\tLoss: 0.118142\n",
            "Train Epoch: 13 [4000/16000 (25%)]\tLoss: 0.000074\n",
            "Train Epoch: 13 [5000/16000 (31%)]\tLoss: 0.007870\n",
            "Train Epoch: 13 [6000/16000 (38%)]\tLoss: 0.000042\n",
            "Train Epoch: 13 [7000/16000 (44%)]\tLoss: 0.002036\n",
            "Train Epoch: 13 [8000/16000 (50%)]\tLoss: 0.000008\n",
            "Train Epoch: 13 [9000/16000 (56%)]\tLoss: 0.000245\n",
            "Train Epoch: 13 [10000/16000 (62%)]\tLoss: 0.000055\n",
            "Train Epoch: 13 [11000/16000 (69%)]\tLoss: 0.003999\n",
            "Train Epoch: 13 [12000/16000 (75%)]\tLoss: 0.000569\n",
            "Train Epoch: 13 [13000/16000 (81%)]\tLoss: 1.243256\n",
            "Train Epoch: 13 [14000/16000 (88%)]\tLoss: 0.000615\n",
            "Train Epoch: 13 [15000/16000 (94%)]\tLoss: 0.000018\n",
            "Train: Average loss: 0.0956, Accuracy: 15370/16000 (0.9606)\n",
            "\n",
            "Val set: Average loss: 0.6066, Accuracy: 2338/4000 (58%)\n",
            "\n",
            "Train Epoch: 14 [0/16000 (0%)]\tLoss: 0.000025\n",
            "Train Epoch: 14 [1000/16000 (6%)]\tLoss: 0.018170\n",
            "Train Epoch: 14 [2000/16000 (12%)]\tLoss: 0.000278\n",
            "Train Epoch: 14 [3000/16000 (19%)]\tLoss: 0.000643\n",
            "Train Epoch: 14 [4000/16000 (25%)]\tLoss: 0.000029\n",
            "Train Epoch: 14 [5000/16000 (31%)]\tLoss: 0.000229\n",
            "Train Epoch: 14 [6000/16000 (38%)]\tLoss: 0.497299\n",
            "Train Epoch: 14 [7000/16000 (44%)]\tLoss: 0.000000\n",
            "Train Epoch: 14 [8000/16000 (50%)]\tLoss: 0.000040\n",
            "Train Epoch: 14 [9000/16000 (56%)]\tLoss: 0.000001\n",
            "Train Epoch: 14 [10000/16000 (62%)]\tLoss: 0.000329\n",
            "Train Epoch: 14 [11000/16000 (69%)]\tLoss: 0.000039\n",
            "Train Epoch: 14 [12000/16000 (75%)]\tLoss: 4.633006\n",
            "Train Epoch: 14 [13000/16000 (81%)]\tLoss: 0.000058\n",
            "Train Epoch: 14 [14000/16000 (88%)]\tLoss: 0.000014\n",
            "Train Epoch: 14 [15000/16000 (94%)]\tLoss: 0.000100\n",
            "Train: Average loss: 0.0746, Accuracy: 15533/16000 (0.9708)\n",
            "\n",
            "Val set: Average loss: 2.0772, Accuracy: 2266/4000 (57%)\n",
            "\n",
            "Train Epoch: 15 [0/16000 (0%)]\tLoss: 0.000842\n",
            "Train Epoch: 15 [1000/16000 (6%)]\tLoss: 0.000003\n",
            "Train Epoch: 15 [2000/16000 (12%)]\tLoss: 0.000083\n",
            "Train Epoch: 15 [3000/16000 (19%)]\tLoss: 0.006783\n",
            "Train Epoch: 15 [4000/16000 (25%)]\tLoss: 0.001609\n",
            "Train Epoch: 15 [5000/16000 (31%)]\tLoss: 0.008684\n",
            "Train Epoch: 15 [6000/16000 (38%)]\tLoss: 0.002315\n",
            "Train Epoch: 15 [7000/16000 (44%)]\tLoss: 0.006419\n",
            "Train Epoch: 15 [8000/16000 (50%)]\tLoss: 0.008049\n",
            "Train Epoch: 15 [9000/16000 (56%)]\tLoss: 0.001507\n",
            "Train Epoch: 15 [10000/16000 (62%)]\tLoss: 0.003865\n",
            "Train Epoch: 15 [11000/16000 (69%)]\tLoss: 0.000006\n",
            "Train Epoch: 15 [12000/16000 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 15 [13000/16000 (81%)]\tLoss: 0.000229\n",
            "Train Epoch: 15 [14000/16000 (88%)]\tLoss: 0.000001\n",
            "Train Epoch: 15 [15000/16000 (94%)]\tLoss: 0.000031\n",
            "Train: Average loss: 0.0672, Accuracy: 15589/16000 (0.9743)\n",
            "\n",
            "Val set: Average loss: 3.2862, Accuracy: 1966/4000 (49%)\n",
            "\n",
            "Train Epoch: 16 [0/16000 (0%)]\tLoss: 0.002600\n",
            "Train Epoch: 16 [1000/16000 (6%)]\tLoss: 0.000001\n",
            "Train Epoch: 16 [2000/16000 (12%)]\tLoss: 0.000031\n",
            "Train Epoch: 16 [3000/16000 (19%)]\tLoss: 0.000003\n",
            "Train Epoch: 16 [4000/16000 (25%)]\tLoss: 0.003150\n",
            "Train Epoch: 16 [5000/16000 (31%)]\tLoss: 0.000042\n",
            "Train Epoch: 16 [6000/16000 (38%)]\tLoss: 0.000886\n",
            "Train Epoch: 16 [7000/16000 (44%)]\tLoss: 0.000861\n",
            "Train Epoch: 16 [8000/16000 (50%)]\tLoss: 0.348393\n",
            "Train Epoch: 16 [9000/16000 (56%)]\tLoss: 0.000004\n",
            "Train Epoch: 16 [10000/16000 (62%)]\tLoss: 0.000040\n",
            "Train Epoch: 16 [11000/16000 (69%)]\tLoss: 0.000002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-b39f9f91c3a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# scheduler = StepLR(optimizer,step_size=1,gamma=)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mtrainmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtvdataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mvalmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtvdataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-8fa158b7dc60>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# print(len(loader))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# print(output,target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-0ec49bf923f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# 56x56\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# 28x28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# 14x14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# 7x7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-0ec49bf923f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdhexBf0xJZy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "HguA_15UMQtk",
        "outputId": "ea6c2021-f8a6-440f-c400-1d442a9926e5"
      },
      "source": [
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN,self).__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "      nn.Conv2d(3,64,3,1,padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(True)\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(True),\n",
        "      nn.Conv2d(64,64,3,1,padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(True),\n",
        "      nn.Conv2d(64,64,3,1,padding=1)\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(True),\n",
        "    )\n",
        "    self.sq1 = nn.Sequential(\n",
        "      nn.AvgPool2d(2)\n",
        "      nn.Conv2d(64,4,1,1)\n",
        "      nn.ReLU(True),\n",
        "      nn.Conv2d(4,64,1,1)\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.avg_pool2d(x,2)\n",
        "    w1 = self.sq1(x)\n",
        "    x = \n",
        "    return output"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-de92bf345b04>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    nn.ReLU(True)\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAumJsfhdj5H"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3,64,3,1,padding=1)\n",
        "    self.conv64 = nn.Sequential(\n",
        "        nn.Conv2d(64,64,3,1,padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "    self.conv128s = nn.Sequential(\n",
        "        nn.Conv2d(64,128,3,1,padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "    self.conv128 = nn.Sequential(\n",
        "        nn.Conv2d(128,128,3,1,padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "    self.conv256s = nn.Sequential(\n",
        "        nn.Conv2d(128,256,3,1,padding=1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "    self.conv256 = nn.Sequential(\n",
        "        nn.Conv2d(256,256,3,1,padding=1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "    self.conv512s = nn.Sequential(\n",
        "        nn.Conv2d(256,512,3,1,padding=1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "    self.conv512 = nn.Sequential(\n",
        "        nn.Conv2d(512,512,3,1,padding=1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(25088,4096),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(0.25),\n",
        "        nn.Linear(4096,4096),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(0.5),\n",
        "        # print('ddkwljef'),\n",
        "        nn.Linear(4096,1000),\n",
        "        nn.Linear(1000,2)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv64(x)\n",
        "    x = self.conv64(x)\n",
        "    x = F.max_pool2d(x,2,stride=2)\n",
        "    x = self.conv128s(x)\n",
        "    x = self.conv128(x)\n",
        "    x = F.max_pool2d(x,2,stride=2)\n",
        "    x = self.conv256s(x)\n",
        "    x = self.conv256(x)\n",
        "    x = self.conv256(x)\n",
        "    x = self.conv256(x)\n",
        "    x = F.max_pool2d(x,2,stride=2)\n",
        "    x = self.conv512s(x)\n",
        "    x = self.conv512(x)\n",
        "    x = self.conv512(x)\n",
        "    x = self.conv512(x)\n",
        "    x = F.max_pool2d(x,2,stride=2)\n",
        "    x = self.conv512(x)\n",
        "    x = self.conv512(x)\n",
        "    x = self.conv512(x)\n",
        "    x = self.conv512(x)\n",
        "    x = F.max_pool2d(x,2,stride=2)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.fc(x)\n",
        "    output = F.log_softmax(x,dim=1)\n",
        "    return output"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuLsTBO2K1yN"
      },
      "source": [
        "fake = np.ndarray((10000,64,64,3),dtype=int)\n",
        "real = np.ndarray((10000,64,64,3),dtype=int)\n",
        "unknown = np.ndarray((5350,64,64,3),dtype=int)\n",
        "for i in range(10000):\n",
        "  fake[i] = cv2.imread('./fake/' + '%04d'%i + '.jpg')\n",
        "for i in range(10000):\n",
        "  real[i] = cv2.imread('./real/' + '%04d'%i + '.jpg')\n",
        "for i in range(5350):\n",
        "  unknown[i] = cv2.imread('./unknown/' + '%04d'%i + '.jpg')\n",
        "plt.imshow(real[50])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "J7aMIjKXiOOW",
        "outputId": "f51b717d-4e45-4be8-d795-96f6d03240a6"
      },
      "source": [
        "n = 4421\n",
        "plt.imshow(fake[n])\n",
        "plt.show()\n",
        "plt.imshow(real[n])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}